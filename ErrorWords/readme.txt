2016.1.19
生成拼音-拼音的句子对：

例：wo3 xi3 huan1 du2 shu1
结果是一行正确拼音，一行构造拼音，
结果存在G_pySentencePair.txt文件里。


2016.1.20
生成拼音-汉字的句子对：
G_tempChnFile.txt文件是存储每个候选拼音句子的候选汉字句子的一个临时文件。
G_pinHanSentPair.txt文件是存储拼音-汉字句子对：
一行拼音，一行汉字句子
即可得到一句拼音-> 拼音候选的句子-> 拼音候选的所有汉字候选的句子。


生成汉字-汉字的句子对：
G_hanHanSentPair.txt文件是存储汉字-汉字对的文件
但是全部运行会特别大，无法显示。

对其进行裁剪，通过百度搜索提取关键词，正在看python的抓取网页，预计明天进行裁剪~


2016.1.21
baidu.py文件是从百度搜索获得所要找的句子的个数，匹配并不是完全匹配，而是包含

_baoli文件是只要有一个不符合，就整句话扔掉，最后只剩下两句，太暴力了~（wo3 xi3 huan1 du2 shu1）

我觉得连接百度搜索这种方法不可取，因为抓取多了，百度会封IP（我的ip已被封~），不利于他人实现~


2016.1.22
运行程序，因为数量过大，出现内存空间不足的情况
导致此情况的原因可能是因为我存入内存的东西过多，因此我将存入内存的list都放在磁盘上，用时现调用
目前解决了此问题~


2016.2.21
完成程序GetPYCandidates_word.py，解决了内存过大问题，采用一个字一个字的方式进行处理，而不是之前整句话一起处理。
随机选取候选，但是随机值大于0.1时才真正随机，否则与原字相同


2016.3.17
每个正句可以得到20句错误句对；
从20句错误句对中抽取出错字对，并转为拼音

总结文件：
IsFit.py                      找出两个文件的错字对，放入一个文件中，并转为拼音。例：八 阿 -> b a1 Null a1 
GetMaxEntropy_pinyin.py       通过训练语料得到拼音句子，声母、韵母分开的。例：1 张 场 例句 -> 1 zh ang ch ang
CalculatePrecision_Word.py    (计算准确率)对于生成的错字文件，对比正确字的文件，得到错字及其位置，和正确答案进行对比。正确答案的格式：00404, 6, 需, 38, 到, 62, 富  
CalculateWordsNum.py          计算一篇文章字的频率；并将 拼音汉字表 进行升级，按字的常用频率进行重排
CalculatePrecision_Pages.py   比较两个格式相同的文件，得到准确率
FileToTwoFiles.py             将一个文件分成两个文件，奇数行一个文件，偶数行一个文件
baidu.py                      抓取百度搜索结果
SentToWord.py                 将一句话中的每个字分开
GetPYCandidates_word.py      【总程序】得到一句字符串的数个错误候选句的形式（以字为单位替换）
GetPYCandidates.py           【不要！】得到一句字符串的数个错误候选句的形式（以整句为单位）